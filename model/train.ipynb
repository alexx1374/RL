{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.wrappers import RecordVideo\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\gym\\wrappers\\record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:289: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Step: 1, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 2, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 3, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 4, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 5, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 6, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 7, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 8, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 9, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 10, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 11, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 12, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 13, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 14, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 15, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 16, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 17, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 18, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 19, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 20, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 21, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 22, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 23, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 24, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 25, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 26, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 27, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 28, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 29, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 30, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 31, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 32, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 33, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 34, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 35, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 36, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 37, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 38, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 39, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 40, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 41, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 42, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 43, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 44, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 45, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 46, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 47, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 48, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 49, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 50, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 51, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 52, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 53, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 54, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 55, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 56, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 57, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 58, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 59, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 60, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n",
      "Episode: 1, Step: 61, Step Reward: 0.0, Total Reward: 0.0, Epsilon: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5808\\608325393.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    124\u001b[0m                 \u001b[0mq_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhuber_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdated_q_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Episode: {episode + 1}, Step: {step}, Step Reward: {reward}, Total Reward: {total_reward}, Epsilon: {epsilon}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1062\u001b[0m               output_gradients))\n\u001b[0;32m   1063\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[0;32m   1064\u001b[0m                           for x in output_gradients]\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1066\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     raise ValueError(\n\u001b[0;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gradient_tape/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRegisterGradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Reshape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_ReshapeGrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m   return [\n\u001b[0;32m    815\u001b[0m       array_ops.reshape(\n\u001b[1;32m--> 816\u001b[1;33m           _IndexedSlicesToTensorNoWarning(grad), array_ops.shape(op.inputs[0])),\n\u001b[0m\u001b[0;32m    817\u001b[0m       \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m   ]\n",
      "\u001b[1;32mc:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input, name, out_type)\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_shape_default_int64\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m       \u001b[0mout_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m       \u001b[0mout_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[0;32m    723\u001b[0m                 input_shape)\n\u001b[0;32m    724\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mout_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input, out_type, name)\u001b[0m\n\u001b[0;32m   9585\u001b[0m         _ctx, \"Shape\", name, input, \"out_type\", out_type)\n\u001b[0;32m   9586\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9587\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9588\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9589\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   9590\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9591\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9592\u001b[0m       return shape_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Environment initialisieren\n",
    "env = gym.make(\"ALE/Pong-v5\", render_mode='rgb_array')\n",
    "\n",
    "\n",
    "# Hyperparameter\n",
    "learning_rate = 0.0001\n",
    "discount_factor = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "batch_size = 32\n",
    "num_episodes = 1000\n",
    "\n",
    "# Netzwerkarchitektur\n",
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(84, 84, 4)),\n",
    "        layers.Conv2D(64, (4, 4), strides=(2, 2), activation='relu'),\n",
    "        layers.Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(env.action_space.n, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Agenteninitialisierung\n",
    "model = create_model()\n",
    "target_model = create_model()\n",
    "target_model.set_weights(model.get_weights())\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "huber_loss = tf.keras.losses.Huber()\n",
    "\n",
    "# Erfahrungsspeicher\n",
    "class ExperienceReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.buffer, batch_size))\n",
    "\n",
    "# Preprocessing-Funktion für den Zustand\n",
    "def preprocess_state(state):\n",
    "    if isinstance(state, np.ndarray):\n",
    "        if len(state.shape) == 3 and state.shape[2] == 3:\n",
    "            gray_state = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n",
    "            resized_state = cv2.resize(gray_state, (84, 84))\n",
    "            return resized_state.astype(np.float32) / 255.0\n",
    "        else:\n",
    "            raise ValueError(\"Der Zustand hat nicht die erwartete Form (Höhe, Breite, Kanäle).\")\n",
    "    elif isinstance(state, tuple) and len(state) == 2:\n",
    "        return preprocess_state(state[0])\n",
    "    else:\n",
    "        raise ValueError(\"Der Zustand ist kein NumPy-Array.\")\n",
    "    \n",
    "# Trainingsmetriken\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "episode_losses = []\n",
    "\n",
    "\n",
    "# Training\n",
    "buffer = ExperienceReplayBuffer()\n",
    "\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    if episode % 50 == 0:\n",
    "        video_env = gym.wrappers.RecordVideo(env, './videos', episode_trigger=lambda episode_id: True, video_length=0)\n",
    "        state = video_env.reset()\n",
    "    else:\n",
    "        state = env.reset()\n",
    "        \n",
    "    state = preprocess_state(state)  # Preprocessing des Zustands\n",
    "    state = np.stack([state] * 4, axis=2)  # Stacke den Zustand viermal entlang der 3. Achse\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0  # Schrittzähler innerhalb der Episode\n",
    "    losses = []\n",
    "\n",
    "    while not done:\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(model.predict(np.expand_dims(state, axis=0), verbose=0))\n",
    "\n",
    "        # Führe den Schritt in der Umgebung aus und erhalte die Rückgabewerte\n",
    "        step_result = env.step(action)\n",
    "\n",
    "        # Extrahiere die ersten vier Rückgabewerte und ignoriere den Rest\n",
    "        next_state, reward, done, _ = step_result[:4]\n",
    "\n",
    "        next_state = preprocess_state(next_state)  # Preprocessing des nächsten Zustands\n",
    "        next_state = np.append(state[:, :, 1:], np.expand_dims(next_state, axis=2), axis=2)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        step += 1  # Schrittzähler erhöhen\n",
    "\n",
    "        if len(buffer.buffer) > batch_size:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "\n",
    "            # Q-Learning\n",
    "            future_rewards = target_model.predict(np.array(next_states), verbose=0)\n",
    "            dones = np.array(dones, dtype=int)\n",
    "\n",
    "            updated_q_values = rewards + discount_factor * np.max(future_rewards, axis=1) * (1 - dones)\n",
    "\n",
    "            masks = tf.one_hot(actions, env.action_space.n)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = model(np.array(states))\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                loss = huber_loss(updated_q_values, q_action)\n",
    "                losses.append(loss.numpy())\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        print(f\"Episode: {episode + 1}, Step: {step}, Step Reward: {reward}, Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "    # Episode abgeschlossen, Metriken aufzeichnen\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_lengths.append(step)\n",
    "    episode_losses.append(np.mean(losses))\n",
    "\n",
    "    # Ausgabe der Schrittanzahl nach Abschluss der Episode\n",
    "    print(f\"Episode {episode + 1} abgeschlossen mit {step} Schritten, Gesamtbelohnung: {total_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "    # Update des Zielnetzwerks\n",
    "    if episode % 10 == 0:\n",
    "        target_model.set_weights(model.get_weights())\n",
    "\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # Plotten der Metriken\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        plt.subplot(3, 1, 1)\n",
    "        plt.plot(episode_rewards, label=\"Total Reward\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Total Reward\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(3, 1, 2)\n",
    "        plt.plot(episode_lengths, label=\"Episode Length\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Length\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(3, 1, 3)\n",
    "        plt.plot(episode_losses, label=\"Loss\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Speichern des fertigen Modells\n",
    "model.save(\"pong_rl_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
