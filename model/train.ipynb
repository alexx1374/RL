{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning Pong Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from gymnasium) (4.12.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gymnasium[atari] in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from gymnasium[atari]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from gymnasium[atari]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from gymnasium[atari]) (4.12.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from gymnasium[atari]) (0.0.4)\n",
      "Requirement already satisfied: shimmy<1.0,>=0.1.0 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.2.1)\n",
      "Requirement already satisfied: ale-py~=0.8.1 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.8.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gymnasium[accept-rom-license] in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from gymnasium[accept-rom-license]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from gymnasium[accept-rom-license]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from gymnasium[accept-rom-license]) (4.12.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
      "Requirement already satisfied: autorom~=0.4.2 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (0.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (8.1.7)\n",
      "Requirement already satisfied: requests in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.32.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (4.66.4)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (0.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from click->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alexp\\onedrive\\studium\\6_semester\\reinforcement_learning\\rl\\.venv\\lib\\site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium\n",
    "%pip install gymnasium[atari]\n",
    "%pip install gymnasium[accept-rom-license]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Pong-v5\", render_mode='rgb_array') # create environment\n",
    "env = gym.wrappers.RecordVideo(env, './videos', episode_trigger=lambda episode_id: True, video_length=0)\n",
    "action_space_n = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "learning_rate = 0.0001\n",
    "discount_factor = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "batch_size = 64\n",
    "num_episodes = 1000\n",
    "frame_skip = 2 # number of frames to skip \n",
    "\n",
    "\n",
    "# model architecture\n",
    "def create_model(action_space_n):\n",
    "    input_layer = layers.Input(shape=(84, 84, 4)) # input shape is 84x84x4\n",
    "    conv1 = layers.Conv2D(32, (8, 8), strides=(4, 4), activation='relu')(input_layer) # 32 filters of 8x8 with stride 4\n",
    "    conv2 = layers.Conv2D(64, (4, 4), strides=(2, 2), activation='relu')(conv1) # 64 filters of 4x4 with stride 2\n",
    "    conv3 = layers.Conv2D(64, (3, 3), strides=(1, 1), activation='relu')(conv2) # 64 filters of 3x3 with stride 1\n",
    "    flatten = layers.Flatten()(conv3) # flatten the output\n",
    "    dense1 = layers.Dense(512, activation='relu')(flatten) # dense layer with 512 units\n",
    "    output_layer = layers.Dense(action_space_n, activation='linear')(dense1) # output layer with action_space_n units\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "# agent initialization\n",
    "model = create_model(action_space_n)\n",
    "target_model = create_model(action_space_n)\n",
    "target_model.set_weights(model.get_weights())\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "huber_loss = tf.keras.losses.Huber()\n",
    "\n",
    "# Experience Replay Buffer\n",
    "class ExperienceReplayBuffer:\n",
    "    def __init__(self, capacity=10000): # capacity of the buffer\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    " \n",
    "    def add(self, state, action, reward, next_state, done): # add experience to the buffer\n",
    "        if len(self.buffer) < self.capacity: \n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done) # add experience to the buffer\n",
    "        self.position = (self.position + 1) % self.capacity # update position\n",
    "\n",
    "    def sample(self, batch_size): # sample experience from the buffer\n",
    "        return zip(*random.sample(self.buffer, batch_size)) # return a batch of experiences\n",
    "\n",
    "# Preprocessing function for the state\n",
    "def preprocess_state(state): \n",
    "    gray_state = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY) # convert to grayscale\n",
    "    resized_state = cv2.resize(gray_state, (84, 84)) # resize to 84x84\n",
    "    return resized_state.astype(np.float32) / 255.0 # normalize\n",
    "\n",
    "# plots\n",
    "def plot_results(episode_rewards, episode_lengths, episode_losses):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(3, 1, 1) \n",
    "    plt.plot(episode_rewards, label=\"Total Reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(episode_lengths, label=\"Episode Length\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Length\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(episode_losses, label=\"Loss\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PrioritizedExperienceReplayBuffer:\n",
    "#     def __init__(self, capacity=10000, alpha=0.6, beta=0.4, beta_increment_per_sampling=0.001, epsilon=0.01):\n",
    "#         self.capacity = capacity\n",
    "#         self.alpha = alpha\n",
    "#         self.beta = beta\n",
    "#         self.beta_increment_per_sampling = beta_increment_per_sampling\n",
    "#         self.epsilon = epsilon\n",
    "#         self.buffer = []\n",
    "#         self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "#         self.position = 0\n",
    "#         self.priorities_sum = 1.0\n",
    "\n",
    "#     def add(self, state, action, reward, next_state, done):\n",
    "#         priority = np.max(self.priorities) if self.buffer else 1.0\n",
    "#         experience = (state, action, reward, next_state, done)\n",
    "#         if len(self.buffer) < self.capacity:\n",
    "#             self.buffer.append(experience)\n",
    "#         else:\n",
    "#             self.buffer[self.position] = experience\n",
    "#         self.priorities[self.position] = priority\n",
    "#         self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         if len(self.buffer) == self.capacity:\n",
    "#             priorities = self.priorities\n",
    "#         else:\n",
    "#             priorities = self.priorities[:self.position]\n",
    "#         probabilities = priorities ** self.alpha\n",
    "#         probabilities /= np.sum(probabilities)  # Normalizing probabilities to sum up to 1\n",
    "#         indices = np.random.choice(len(self.buffer), size=batch_size, p=probabilities)\n",
    "#         samples = [self.buffer[idx] for idx in indices]\n",
    "#         weights = (len(self.buffer) * probabilities[indices]) ** (-self.beta)\n",
    "#         weights /= weights.max()\n",
    "#         self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
    "#         return samples, indices, weights\n",
    "\n",
    "#     def update_priorities(self, indices, priorities):\n",
    "#         self.priorities[indices] = priorities\n",
    "#         self.priorities_sum = np.sum(self.priorities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training metrics\n",
    "# episode_rewards = []\n",
    "# episode_lengths = []\n",
    "# episode_losses = []\n",
    "\n",
    "# buffer = PrioritizedExperienceReplayBuffer(capacity=10000, alpha=alpha, beta=beta)\n",
    "\n",
    "# record_env = None\n",
    "# videos_dir = \"videos\"\n",
    "# os.makedirs(videos_dir, exist_ok=True)\n",
    "\n",
    "# best_reward = -float('inf')\n",
    "\n",
    "# for episode in range(num_episodes):\n",
    "#     if (episode + 1) % 25 == 0:\n",
    "#         if record_env is not None:\n",
    "#             record_env.close()\n",
    "#         video_path = os.path.join(videos_dir, f\"episode_{episode + 1}\")\n",
    "#         record_env = gym.wrappers.RecordVideo(env, video_path, episode_trigger=lambda episode_id: True, video_length=0)\n",
    "#     else:\n",
    "#         env.reset()\n",
    "\n",
    "#     state, info = env.reset()\n",
    "#     state = preprocess_state(state)\n",
    "#     state = np.stack([state] * 4, axis=2)\n",
    "#     done = False\n",
    "#     total_reward = 0\n",
    "#     step = 0\n",
    "#     losses = []\n",
    "\n",
    "#     while not done:\n",
    "#         if np.random.rand() <= epsilon:\n",
    "#             action = env.action_space.sample()\n",
    "#         else:\n",
    "#             action = np.argmax(model.predict(np.expand_dims(state, axis=0), verbose=0))\n",
    "\n",
    "#         for _ in range(frame_skip):\n",
    "#             next_state, reward, done, truncated, info = env.step(action)\n",
    "#             done = done or truncated\n",
    "#             total_reward += reward\n",
    "#             if done:\n",
    "#                 break\n",
    "\n",
    "#         next_state, reward, done, truncated, info = env.step(action)\n",
    "#         done = done or truncated\n",
    "\n",
    "#         next_state = preprocess_state(next_state)\n",
    "#         next_state = np.append(state[:, :, 1:], np.expand_dims(next_state, axis=2), axis=2)\n",
    "\n",
    "#         total_reward += reward\n",
    "\n",
    "#         buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "#         state = next_state\n",
    "#         step += 1\n",
    "\n",
    "#         if len(buffer.buffer) > batch_size:\n",
    "#             samples, indices, weights = buffer.sample(batch_size)\n",
    "#             states, actions, rewards, next_states, dones = zip(*samples)\n",
    "\n",
    "#             future_rewards = target_model.predict(np.array(next_states), verbose=0)\n",
    "#             dones = np.array(dones, dtype=int)\n",
    "#             updated_q_values = rewards + discount_factor * np.max(future_rewards, axis=1) * (1 - dones)\n",
    "\n",
    "#             masks = tf.one_hot(actions, action_space_n)\n",
    "\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 q_values = model(np.array(states))\n",
    "#                 q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "#                 loss = huber_loss(updated_q_values, q_action)\n",
    "#                 weighted_loss = tf.reduce_mean(loss * weights)  # Weighted loss\n",
    "#                 losses.append(weighted_loss.numpy())\n",
    "\n",
    "#             grads = tape.gradient(weighted_loss, model.trainable_variables)\n",
    "#             optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "#             buffer.update_priorities(indices, loss.numpy())\n",
    "\n",
    "#     episode_rewards.append(total_reward)\n",
    "#     episode_lengths.append(step)\n",
    "#     episode_losses.append(np.mean(losses))\n",
    "\n",
    "#     print(f\"Episode {episode + 1} abgeschlossen mit {step} Schritten, Gesamtbelohnung: {total_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "#     if episode % 10 == 0:\n",
    "#         target_model.set_weights(model.get_weights())\n",
    "\n",
    "#     epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "#     if total_reward > best_reward:\n",
    "#         best_reward = total_reward\n",
    "#         model.save(\"best_pong_v5_rl_model.keras\")\n",
    "\n",
    "#     if (episode + 1) % 100 == 0:\n",
    "#         model.save(f\"pong_v5_rl_model_episode_{episode + 1}.keras\")\n",
    "\n",
    "#     if (episode + 1) % 5 == 0:\n",
    "#         plot_results(episode_rewards, episode_lengths, episode_losses)\n",
    "\n",
    "# if record_env is not None:\n",
    "#     record_env.close()\n",
    "\n",
    "# model.save(\"pong_rl_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18676\\2672653633.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[0mq_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhuber_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdated_q_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m#print(f\"Episode: {episode + 1}, Step: {step}, Step Reward: {reward}, Total Reward: {total_reward}, Epsilon: {epsilon}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1062\u001b[0m               output_gradients))\n\u001b[0;32m   1063\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[0;32m   1064\u001b[0m                           for x in output_gradients]\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1066\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     raise ValueError(\n\u001b[0;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gradient_tape/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    590\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m           data_format=data_format),\n\u001b[1;32m--> 594\u001b[1;33m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[0m\u001b[0;32m    595\u001b[0m           \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m           \u001b[0mshape_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexp\\OneDrive\\Studium\\6_Semester\\Reinforcement_Learning\\RL\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1495\u001b[0m         data_format, \"dilations\", dilations)\n\u001b[0;32m   1496\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1497\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1499\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1500\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1501\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1502\u001b[0m       return conv2d_backprop_filter_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training metrics\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "episode_losses = []\n",
    "\n",
    "buffer = ExperienceReplayBuffer()\n",
    "best_reward = -float('inf') \n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.reset() \n",
    "    state, info = env.reset()\n",
    "    state = preprocess_state(state)  # Preprocessing of the state\n",
    "    state = np.stack([state] * 4, axis=2)  # Stacking of the state 4 times\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0  # step counter for the episode\n",
    "    losses = []\n",
    "\n",
    "    while not done: # loop through steps\n",
    "        if np.random.rand() <= epsilon: # epsilon greedy policy\n",
    "            action = env.action_space.sample() # take random action\n",
    "        else:\n",
    "            action = np.argmax(model.predict(np.expand_dims(state, axis=0), verbose=0))\n",
    "\n",
    "        # Frame skipping logic\n",
    "        # for _ in range(frame_skip):\n",
    "        #     next_state, reward, done, truncated, info = env.step(action)\n",
    "        #     done = done or truncated\n",
    "        #     total_reward += reward\n",
    "        #     if done:\n",
    "        #         break\n",
    "\n",
    "        next_state, reward, done, truncated, info = env.step(action) # take action in the environment\n",
    "        done = done or truncated  # check if the episode is done or truncated\n",
    "\n",
    "        next_state = preprocess_state(next_state)  \n",
    "        next_state = np.append(state[:, :, 1:], np.expand_dims(next_state, axis=2), axis=2)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        step += 1  \n",
    "\n",
    "        if len(buffer.buffer) > batch_size:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "\n",
    "            # Q-Learning\n",
    "            future_rewards = target_model.predict(np.array(next_states), verbose=0) # predict future rewards with target model\n",
    "            dones = np.array(dones, dtype=int) # convert dones to int\n",
    "\n",
    "            updated_q_values = rewards + discount_factor * np.max(future_rewards, axis=1) * (1 - dones) # calculate updated q values\n",
    "\n",
    "            masks = tf.one_hot(actions, action_space_n) # create masks \n",
    "\n",
    "            with tf.GradientTape() as tape: # calculate loss\n",
    "                q_values = model(np.array(states))\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                loss = huber_loss(updated_q_values, q_action)\n",
    "                losses.append(loss.numpy())\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        #print(f\"Episode: {episode + 1}, Step: {step}, Step Reward: {reward}, Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "    episode_rewards.append(total_reward) #append total reward to list\n",
    "    episode_lengths.append(step) #append step to list\n",
    "    episode_losses.append(np.mean(losses)) #append loss to list\n",
    "\n",
    "    print(f\"Episode {episode + 1} abgeschlossen mit {step} Schritten, Gesamtbelohnung: {total_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        target_model.set_weights(model.get_weights())\n",
    "\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    if total_reward > best_reward:  # save best model\n",
    "        best_reward = total_reward\n",
    "        model.save(\"best_pong_v5_rl_model.keras\")\n",
    "\n",
    "    if (episode + 1) % 100 == 0: #save model every 100 episodes\n",
    "        model.save(f\"pong_v5_rl_model_episode_{episode + 1}.keras\")\n",
    "\n",
    "    if (episode + 1) % 50 == 0: \n",
    "        plot_results(episode_rewards, episode_lengths, episode_losses)      \n",
    "        plt.savefig(f\"pong_v5_rl_model_episode_{episode + 1}.png\") \n",
    "\n",
    "model.save(\"pong_rl_model.keras\") #save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
